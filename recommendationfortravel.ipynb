{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":10484543,"sourceType":"datasetVersion","datasetId":6491505}],"dockerImageVersionId":30839,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\nimport ast\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session\nfrom IPython.display import display\nfrom IPython.display import Image","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-01-18T11:38:46.049197Z","iopub.execute_input":"2025-01-18T11:38:46.049559Z","iopub.status.idle":"2025-01-18T11:38:47.271045Z","shell.execute_reply.started":"2025-01-18T11:38:46.049524Z","shell.execute_reply":"2025-01-18T11:38:47.270082Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/travel/users.csv\n/kaggle/input/travel/places.csv\n/kaggle/input/travel/user_interactions.csv\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"def read_data():\n    places_df = pd.read_csv(\"/kaggle/input/travel/places.csv\")\n    user_interactions_df = pd.read_csv(\"/kaggle/input/travel/user_interactions.csv\")\n    users_df = pd.read_csv(\"/kaggle/input/travel/users.csv\")\n    return places_df, user_interactions_df, users_df","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-18T07:27:20.051703Z","iopub.execute_input":"2025-01-18T07:27:20.052266Z","iopub.status.idle":"2025-01-18T07:27:20.073097Z","shell.execute_reply.started":"2025-01-18T07:27:20.052218Z","shell.execute_reply":"2025-01-18T07:27:20.072040Z"}},"outputs":[],"execution_count":23},{"cell_type":"code","source":"original_places_df, original_user_interactions_df, original_users_df = read_data()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-18T07:27:20.075166Z","iopub.execute_input":"2025-01-18T07:27:20.075566Z","iopub.status.idle":"2025-01-18T07:27:20.104555Z","shell.execute_reply.started":"2025-01-18T07:27:20.075525Z","shell.execute_reply":"2025-01-18T07:27:20.103429Z"}},"outputs":[],"execution_count":24},{"cell_type":"code","source":"!pip install lightfm","metadata":{"trusted":true,"execution":{"iopub.status.idle":"2025-01-21T19:09:56.858006Z","shell.execute_reply.started":"2025-01-21T19:09:39.004477Z","shell.execute_reply":"2025-01-21T19:09:56.856169Z"}},"outputs":[{"name":"stdout","text":"  Building wheel for lightfm (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Created wheel for lightfm: filename=lightfm-1.17-cp310-cp310-linux_x86_64.whl size=808329 sha256=a6a756a573cfa68de9c55f295226fa62bbd705e5c6ece6cd80d61faaa5aaca10\n  Stored in directory: /root/.cache/pip/wheels/4f/9b/7e/0b256f2168511d8fa4dae4fae0200fdbd729eb424a912ad636\nSuccessfully built lightfm\nInstalling collected packages: lightfm\nSuccessfully installed lightfm-1.17\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"def preprocess_places_data(places_df):\n    tags = list(set([i.strip() for i in ','.join(places_df['tags']).split(',')]))\n    for tag in tags:\n        places_df[tag] = places_df['tags'].apply(lambda x: 1 if tag in x else 0)\n    category_features = pd.get_dummies(places_df['category'], prefix='category', dtype=int)\n    places_df = pd.concat([places_df, category_features], axis=1)\n    return places_df\n\n\ndef preprocess_user_interaction_data(user_interactions_df):\n    interest_strength = {\"like\": 1,\n                         \"visit\" : 3,\n                         \"add_to_list\": 2\n                         }\n    \n    user_interactions_df['weighted_interaction'] = user_interactions_df['interaction_type'].apply(lambda x: interest_strength[x])\n    # display(user_interactions_df)\n\n    # Create interaction matrix\n\n    return user_interactions_df\n\ndef preprocess_users_data(user_df):\n    user_df['list_of_places'] = user_df['list_of_places'].apply(lambda x: ast.literal_eval(x))\n    return user_df\n\nplaces_df = preprocess_places_data(original_places_df.copy())\nuser_interaction_df = preprocess_user_interaction_data(original_user_interactions_df.copy())\nuser_df = preprocess_users_data(original_users_df.copy())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-21T19:09:57.769092Z","iopub.execute_input":"2025-01-21T19:09:57.769584Z","iopub.status.idle":"2025-01-21T19:09:57.802377Z","shell.execute_reply.started":"2025-01-21T19:09:57.769540Z","shell.execute_reply":"2025-01-21T19:09:57.800635Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-3-d4d7e20e317f>\u001b[0m in \u001b[0;36m<cell line: 27>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0muser_df\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m \u001b[0mplaces_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreprocess_places_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moriginal_places_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0muser_interaction_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreprocess_user_interaction_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moriginal_user_interactions_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0muser_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreprocess_users_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moriginal_users_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'original_places_df' is not defined"],"ename":"NameError","evalue":"name 'original_places_df' is not defined","output_type":"error"}],"execution_count":3},{"cell_type":"markdown","source":"There are no data that has list_of_places lower than 5, so we dont need to worry about no recommendation/zero-start\nI am assuming there are no redundant data as in a person that has marked one place as liked, visited and add_to_list","metadata":{}},{"cell_type":"markdown","source":"NO NA in all 3 datasets so no need to handle Null values","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom sklearn.metrics.pairwise import cosine_similarity\nfrom sklearn.preprocessing import MultiLabelBinarizer, StandardScaler\nfrom collections import defaultdict\nimport time\nimport ast\nfrom typing import List, Dict, Set, Tuple\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.decomposition import TruncatedSVD\nfrom scipy.sparse import csr_matrix\nfrom lightfm import LightFM\nfrom lightfm.data import Dataset\n\nfrom IPython.display import display\nfrom IPython.display import Image\n\nclass PlaceRecommender:\n    def __init__(self):\n        self.users_df = None\n        self.places_df = None\n        self.interactions_df = None\n        self.place_features = None\n        self.user_profiles = None\n        # self.mlb = MultiLabelBinarizer()\n        self.train_interactions = pd.DataFrame()\n        self.test_interactions = pd.DataFrame()\n        self.svd_model = None\n        self.interaction_matrix = None\n        self.user_factors = None\n        self.place_factors = None\n        # self.popularity = None ???? We can add this too for recommending highly popular places\n        self.lightfm_model = None\n        self.lightfm_dataset = None\n        self.user_features_matrix = None\n        self.item_features_matrix = None\n        self.user_id_map = {}\n        self.place_id_map = {}\n    \n    \n    def preprocess_places_data(self, places_df):\n        tags = list(set([i.strip() for i in ','.join(places_df['tags']).split(',')]))\n        for tag in tags:\n            places_df[tag] = places_df['tags'].apply(lambda x: 1 if tag in x else 0)\n        category_features = pd.get_dummies(places_df['category'], prefix='category', dtype=int)\n        tags.extend(list(category_features.columns.values))\n        places_df = pd.concat([places_df, category_features], axis=1)\n        scaler = StandardScaler()\n        place_features = places_df[tags]\n        place_features = pd.DataFrame(\n            scaler.fit_transform(place_features),\n            index = place_features.index,\n            columns=place_features.columns\n        )\n        \n        return places_df, place_features\n\n\n    def preprocess_user_interaction_data(self, user_interactions_df):\n        interest_strength = {\"like\": 1,\n                             \"visit\" : 3,\n                             \"add_to_list\": 2\n                             }\n        \n        user_interactions_df['weighted_interaction'] = user_interactions_df['interaction_type'].apply(lambda x: interest_strength[x])\n        # Maybe interaction matrix?\n        return user_interactions_df\n    \n    def preprocess_users_data(self, user_df):\n        user_df['list_of_places'] = user_df['list_of_places'].apply(lambda x: ast.literal_eval(x))\n        return user_df\n\n    def preprocess_lightfm_data(self):\n        self.lightfm_dataset = Dataset()\n    \n        # Get all possible user feature values\n        user_features = []\n        for column in self.users_df.columns[1:]:  # Exclude user_id\n            user_features.extend(self.users_df[column].astype(str).unique())\n        \n        # Get all possible item feature values\n        item_features = list(self.places_df['category'].unique())\n        # Add all unique tags\n        all_tags = set()\n        for tags in self.places_df['tags']:\n            if isinstance(tags, list):\n                all_tags.update(tags)\n            else:  # If tags is a string\n                all_tags.update(str(tags).split(','))\n        item_features.extend(list(all_tags))\n        \n        # Fit the dataset with all possible feature values\n        self.lightfm_dataset.fit(\n            users=self.interactions_df['user_id'].unique(),\n            items=self.interactions_df['place_id'].unique(),\n            user_features=user_features,\n            item_features=item_features\n        )\n        \n        # Build interaction matrix\n        (interactions, weights) = self.lightfm_dataset.build_interactions(\n            ((row['user_id'], row['place_id'], row['weighted_interaction']) \n             for _, row in self.train_interactions.iterrows())\n        )\n        \n        # Build user features\n        user_features_list = []\n        for user_id in self.interactions_df['user_id'].unique():\n            if user_id in self.users_df['user_id'].values:\n                user_row = self.users_df[self.users_df['user_id'] == user_id].iloc[0]\n                # Convert all values to strings\n                features = [str(val) for val in user_row.values[1:]]\n                user_features_list.append((user_id, features))\n        \n        user_features = self.lightfm_dataset.build_user_features(user_features_list)\n        \n        # Build item features\n        item_features_list = []\n        for place_id in self.interactions_df['place_id'].unique():\n            if place_id in self.places_df.index:\n                place_row = self.places_df.loc[place_id]\n                features = []\n                features.append(str(place_row['category']))\n                if isinstance(place_row['tags'], list):\n                    features.extend([str(tag) for tag in place_row['tags']])\n                else:\n                    features.extend([str(tag.strip()) for tag in place_row['tags'].split(',')]) #this is not needed\n                item_features_list.append((place_id, features))\n        \n        item_features = self.lightfm_dataset.build_item_features(item_features_list)\n        \n        return interactions, weights, user_features, item_features\n\n\n    def train_lightfm(self):\n        \"\"\"Train LightFM model.\"\"\"\n        print(\"Training LightFM model...\")\n        self.lightfm_model = LightFM(\n            learning_rate=0.005,\n            loss='warp',\n            no_components=50,\n            random_state=42\n        )\n        interactions, weights, user_features, item_features = self.preprocess_lightfm_data()\n        self.lightfm_model.fit(\n            interactions=interactions,\n            sample_weight=weights,\n            user_features=user_features,\n            item_features=item_features,\n            epochs=30,\n            verbose=True\n        )\n        \n        self.user_features_matrix = user_features\n        self.item_features_matrix = item_features\n        \n    def get_lightfm_scores(self, user_id: int) -> np.ndarray:\n        \"\"\"Get recommendation scores from LightFM model.\"\"\"\n        if user_id not in self.user_id_map:\n            return np.zeros(len(self.places_df))\n            \n        # Get internal user ID\n        user_idx = self.lightfm_dataset.mapping()[0][user_id]\n        \n        # Predict scores for all items\n        scores = self.lightfm_model.predict(\n            user_ids=[user_idx],\n            item_ids=np.arange(len(self.places_df)),\n            user_features=self.user_features_matrix,\n            item_features=self.item_features_matrix\n        )\n        \n        return scores\n        \n    def load_and_preprocess_data(self, users_data, places_data, interactions_data, test_size=0.2):\n        \"\"\"Load and preprocess all necessary data.\"\"\"\n        # Load dataframes\n        self.users_df = self.preprocess_users_data(users_data)\n        self.places_df, self.place_features = self.preprocess_places_data(places_data)\n        self.interactions_df = self.preprocess_user_interaction_data(interactions_data)\n        grouped_user_interactions_df = self.interactions_df.groupby('user_id')\n        for _, group in grouped_user_interactions_df:\n            if len(group) > 2: #We would need to worry about zero start users.\n                train, test = train_test_split(group, test_size=test_size, random_state=42)\n                # display(train, test)\n                self.train_interactions = pd.concat([self.train_interactions, train])\n                self.test_interactions = pd.concat([self.test_interactions, test])\n                \n        user_indices = self.interactions_df['user_id'].astype('category').cat.codes #we might need this for data that is not sequential, right now is fine\n        place_indices = self.interactions_df['place_id'].astype('category').cat.codes\n        # display(place_indices, place_indices.unique().shape)\n\n        self.interaction_matrix = csr_matrix((self.interactions_df['weighted_interaction'], (user_indices, place_indices)), shape=(len(user_indices.unique()), len(place_indices)))\n        # display(self.interaction_matrix.shape)\n        self.svd_model = TruncatedSVD(n_components=50, random_state=42)\n        self.user_factors = self.svd_model.fit_transform(self.interaction_matrix)\n        self.place_factors = self.svd_model.components_.T\n        ### We can also add how popular a place is?\n        self.train_lightfm()\n    \n        \n    \n    def build_user_profiles(self):\n        \n        self.user_profiles = defaultdict(lambda: np.zeros(self.place_features.shape[1]))\n        \n        # Process explicit interactions (likes and add_to_list)\n        for _, row in self.interactions_df.iterrows():\n            user_id = row['user_id']\n            place_id = row['place_id']\n            interaction_type = row['interaction_type']\n            \n            # Weight different interaction types\n            weight = row['weighted_interaction']\n            \n            # Update user profile\n            if place_id in self.places_df.index:\n                place_feature_values = self.place_features.iloc[place_id-1].values.astype(np.float64)\n                self.user_profiles[user_id] += weight * place_feature_values\n        \n        # Normalize\n        for user_id in self.user_profiles:\n            profile = self.user_profiles[user_id]\n            if np.any(profile):\n                self.user_profiles[user_id] = profile / np.linalg.norm(profile)\n    \n    def get_recommendations(self, user_id, n_recommendations= 1):\n        \"\"\"Weight to cf and similarities is 0.4 and 0.6 respectively, these hyperparameters can be tuned to find a better answer\"\"\"\n        start_time = time.time()\n        \n        if user_id not in self.user_profiles:\n            return []\n        \n        # Calculate similarity between user profile and all places\n        cf_scores = np.zeros(len(self.places_df))\n        user_profile = self.user_profiles[user_id]\n        similarities = cosine_similarity([user_profile], self.place_features)[0]\n        place_indices = self.interactions_df['place_id'].astype('category').cat.codes.unique()\n        user_idx = self.interactions_df['user_id'].astype('category').cat.codes[self.interactions_df['user_id'] == user_id].iloc[0]\n        cf_predictions = np.dot(self.user_factors[user_idx], self.place_factors.T)\n        for idx, score in zip(place_indices, cf_predictions):\n            cf_scores[idx] = score\n        lightfm_scores = self.get_lightfm_scores(user_id)\n        # display(similarities.shape, cf_scores.shape)\n        combined_scores = 0.3 * similarities + 0.4 * cf_scores + 0.3 * lightfm_scores\n        # print(combined_scores)\n        \n        # Get places the user hasn't interacted with\n        user_interactions = set(self.train_interactions[self.train_interactions['user_id'] == user_id]['place_id'])\n        # display(user_interactions)\n        # Creating list of (place_id, similarity) tuples for places user hasn't interacted with\n        place_similarities = [\n            (place_id, sim) for place_id, sim in enumerate(combined_scores, 1)\n            if place_id not in user_interactions\n        ]\n        # display(place_similarities)\n        #####HMMM UNSURE WHY Chaning learning rate has no effect or changinf weight of each \n        # Sorting by similarity\n        recommendations = sorted(place_similarities, key=lambda x: x[1], reverse=True)[:n_recommendations]\n        # print(recommendations)\n        \n        detailed_recommendations = []\n        for place_id, similarity in recommendations:\n            place = self.places_df.iloc[place_id-1]\n            detailed_recommendations.append({\n                'place_id': place_id,\n                'place_name': place['place_name'],\n                'category': place['category'],\n                'tags': place['tags'],\n                'location': place['location'],\n                'similarity_score': similarity,\n                'response_time': time.time() - start_time\n            })\n        # display(detailed_recommendations)\n        return detailed_recommendations\n\n    def evaluate_precision_at_k(self, test_users, k):\n        precisions = []\n        \n        for user_id in self.test_interactions['user_id'].unique():\n            recommendations = self.get_recommendations(user_id, k)\n            if not recommendations:\n                continue\n                \n            # Get test set places for this user\n            test_places = set(self.test_interactions[self.test_interactions['user_id'] == user_id]['place_id'])\n            \n            # Calculate precision\n            recommended_places = {rec['place_id'] for rec in recommendations}\n            if recommended_places:\n                precision = len(test_places.intersection(recommended_places)) / len(recommended_places)\n                precisions.append(precision)\n        return np.mean(precisions) if precisions else 0.0\n\n    def evaluate_recall_at_k(self, test_users, k):\n        recalls = []\n        \n        for user_id in self.test_interactions['user_id'].unique():\n            recommendations = self.get_recommendations(user_id, k)\n            if not recommendations:\n                continue\n                \n            # Get test set places for this user\n            test_places = set(self.test_interactions[self.test_interactions['user_id'] == user_id]['place_id'])\n            \n            if test_places:\n                recommended_places = {rec['place_id'] for rec in recommendations}\n                recall = len(test_places.intersection(recommended_places)) / len(test_places)\n                recalls.append(recall)\n        \n        return np.mean(recalls) if recalls else 0.0\n\n    def evaluate_map(self, test_users, k):\n        ap_scores = []\n        \n        for user_id in self.test_interactions['user_id'].unique():\n            recommendations = self.get_recommendations(user_id, k)\n            if not recommendations:\n                continue\n                \n            test_places = set(self.test_interactions[self.test_interactions['user_id'] == user_id]['place_id'])\n            \n            if not test_places:\n                continue\n            relevant_count = 0\n            precisions = []\n            \n            for i, rec in enumerate(recommendations, 1):\n                if rec['place_id'] in test_places:\n                    relevant_count += 1\n                    precisions.append(relevant_count / i)\n            \n            if precisions:\n                ap_scores.append(sum(precisions) / len(test_places))\n        \n        return np.mean(ap_scores) if ap_scores else 0.0\n\n\n    def evaluate_response_time(self, test_users, k):\n        response_times = []\n        \n        for user_id in test_users:\n            start_time = time.time()\n            self.get_recommendations(user_id, n_recommendations=k)\n            response_time = time.time() - start_time\n            response_times.append(response_time)\n        \n        return {\n            'mean_response_time': np.mean(response_times),\n            'max_response_time': np.max(response_times),\n            'min_response_time': np.min(response_times),\n            'std_response_time': np.std(response_times)\n        }\n\n    def run_comprehensive_evaluation(self, test_users, k=1):\n        results = {\n            'precision_at_k': self.evaluate_precision_at_k(test_users, k),\n            'recall_at_k': self.evaluate_recall_at_k(test_users, k),\n            'mean_average_precision': self.evaluate_map(test_users, k),\n            'response_time_metrics': self.evaluate_response_time(test_users, k)\n        }\n        return results\n\ndef main():\n    # Initialize recommender\n    k = 10\n    recommender = PlaceRecommender()\n    \n    # Load and preprocess data\n    places_df = pd.read_csv(\"/kaggle/input/travel/places.csv\")\n    user_interactions_df = pd.read_csv(\"/kaggle/input/travel/user_interactions.csv\")\n    users_df = pd.read_csv(\"/kaggle/input/travel/users.csv\")\n    \n    recommender.load_and_preprocess_data(users_df, places_df, user_interactions_df)\n    recommender.build_user_profiles()\n    test_users = list(range(1,10)) \n    \n    evaluation_results = recommender.run_comprehensive_evaluation(test_users, k)\n    # display(evaluation_results)\n    # Print evaluation results\n    print(\"\\nEvaluation Results:\")\n    print(f\"Precision@{k}: {evaluation_results['precision_at_k']:.3f}\")\n    print(f\"Recall@{k} {evaluation_results['recall_at_k']:.3f}\")\n    print(f\"Mean Average Precision: {evaluation_results['mean_average_precision']:.3f}\")\n    print(\"\\nResponse Time Metrics:\")\n    for metric, value in evaluation_results['response_time_metrics'].items():\n        print(f\"{metric}: {value*1000:.2f}ms\")\n\nif __name__ == \"__main__\":\n    main()    ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-21T19:39:27.615856Z","iopub.execute_input":"2025-01-21T19:39:27.616355Z","iopub.status.idle":"2025-01-21T19:39:28.460058Z","shell.execute_reply.started":"2025-01-21T19:39:27.616295Z","shell.execute_reply":"2025-01-21T19:39:28.458823Z"}},"outputs":[{"name":"stdout","text":"Training LightFM model...\n","output_type":"stream"},{"name":"stderr","text":"Epoch: 100%|██████████| 30/30 [00:00<00:00, 629.97it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nEvaluation Results:\nPrecision@10: 0.032\nRecall@10 0.216\nMean Average Precision: 0.148\n\nResponse Time Metrics:\nmean_response_time: 3.22ms\nmax_response_time: 3.78ms\nmin_response_time: 3.06ms\nstd_response_time: 0.21ms\n","output_type":"stream"}],"execution_count":23},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}